{
  "bug_id": "94328",
  "issue_url": "https://github.com/llvm/llvm-project/issues/94328",
  "bug_type": "crash",
  "base_commit": "098bd842a7e50853fa231f8b73c24ec5006fe063",
  "knowledge_cutoff": "2024-06-04T09:16:11Z",
  "lit_test_dir": [
    "llvm/test/Transforms/LoopVectorize"
  ],
  "hints": {
    "fix_commit": "008df3cf85e9bb1532c079bfd7a7a00e90e0a3c6",
    "components": [
      "LoopVectorize"
    ],
    "files": [
      "llvm/lib/Transforms/Vectorize/LoopVectorize.cpp"
    ],
    "bug_location_lineno": {
      "llvm/lib/Transforms/Vectorize/LoopVectorize.cpp": [
        [
          3914,
          3932
        ],
        [
          9516,
          9521
        ]
      ]
    },
    "bug_location_funcname": {
      "llvm/lib/Transforms/Vectorize/LoopVectorize.cpp": [
        "LoopVectorizationCostModel::collectLoopUniforms",
        "VPReplicateRecipe::execute"
      ]
    }
  },
  "patch": "commit 008df3cf85e9bb1532c079bfd7a7a00e90e0a3c6\nAuthor: Florian Hahn <flo@fhahn.com>\nDate:   Fri Jul 19 12:02:25 2024 +0100\n\n    [LV] Check isPredInst instead of isScalarWithPred in uniform analysis. (#98892)\n    \n    Any instruction marked as uniform will result in a uniform\n    VPReplicateRecipe. If it requires predication, it will be placed in a\n    replicate region, even if isScalarWithPredication returns false.\n    \n    Check isPredicatedInst instead of isScalarWithPredication to avoid\n    generating uniform VPReplicateRecipes placed inside a replicate region.\n    This fixes an assertion when using scalable VFs.\n    \n    Fixes https://github.com/llvm/llvm-project/issues/80416.\n    Fixes https://github.com/llvm/llvm-project/issues/94328.\n    Fixes https://github.com/llvm/llvm-project/issues/99625.\n    \n    PR: https://github.com/llvm/llvm-project/pull/98892\n\ndiff --git a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp\nindex fbca4cdcbcfc..cceed75aa50b 100644\n--- a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp\n+++ b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp\n@@ -3914,19 +3914,19 @@ void LoopVectorizationCostModel::collectLoopUniforms(ElementCount VF) {\n   SetVector<Instruction *> Worklist;\n \n   // Add uniform instructions demanding lane 0 to the worklist. Instructions\n-  // that are scalar with predication must not be considered uniform after\n+  // that require predication must not be considered uniform after\n   // vectorization, because that would create an erroneous replicating region\n   // where only a single instance out of VF should be formed.\n-  // TODO: optimize such seldom cases if found important, see PR40816.\n   auto addToWorklistIfAllowed = [&](Instruction *I) -> void {\n     if (isOutOfScope(I)) {\n       LLVM_DEBUG(dbgs() << \"LV: Found not uniform due to scope: \"\n                         << *I << \"\\n\");\n       return;\n     }\n-    if (isScalarWithPredication(I, VF)) {\n-      LLVM_DEBUG(dbgs() << \"LV: Found not uniform being ScalarWithPredication: \"\n-                        << *I << \"\\n\");\n+    if (isPredicatedInst(I)) {\n+      LLVM_DEBUG(\n+          dbgs() << \"LV: Found not uniform due to requiring predication: \" << *I\n+                 << \"\\n\");\n       return;\n     }\n     LLVM_DEBUG(dbgs() << \"LV: Found uniform instruction: \" << *I << \"\\n\");\n@@ -9516,6 +9516,8 @@ void VPInterleaveRecipe::execute(VPTransformState &State) {\n void VPReplicateRecipe::execute(VPTransformState &State) {\n   Instruction *UI = getUnderlyingInstr();\n   if (State.Instance) { // Generate a single instance.\n+    assert((State.VF.isScalar() || !isUniform()) &&\n+           \"uniform recipe shouldn't be predicated\");\n     assert(!State.VF.isScalable() && \"Can't scalarize a scalable vector\");\n     State.ILV->scalarizeInstruction(UI, this, *State.Instance, State);\n     // Insert scalar instance packing it into a vector.\n",
  "tests": [
    {
      "file": "llvm/test/Transforms/LoopVectorize/AArch64/divs-with-scalable-vfs.ll",
      "commands": [
        "opt -p loop-vectorize -mtriple aarch64 -mcpu=neoverse-v1 -S %s"
      ],
      "tests": [
        {
          "test_name": "udiv_urem_feeding_gep",
          "test_body": "define void @udiv_urem_feeding_gep(i64 %x, ptr %dst, i64 %N) {\nentry:\n  %mul.1.i = mul i64 %x, %x\n  %mul.2.i = mul i64 %mul.1.i, %x\n  br label %loop\n\nloop:                                             ; preds = %loop, %entry\n  %iv = phi i64 [ 0, %entry ], [ %iv.next, %loop ]\n  %div.i = udiv i64 %iv, %mul.2.i\n  %rem.i = urem i64 %iv, %mul.2.i\n  %div.1.i = udiv i64 %rem.i, %mul.1.i\n  %rem.1.i = urem i64 %rem.i, %mul.1.i\n  %div.2.i = udiv i64 %rem.1.i, %x\n  %rem.2.i = urem i64 %rem.1.i, %x\n  %mul.i = mul i64 %x, %div.i\n  %add.i = add i64 %mul.i, %div.1.i\n  %mul.1.i9 = mul i64 %add.i, %x\n  %add.1.i = add i64 %mul.1.i9, %div.2.i\n  %mul.2.i11 = mul i64 %add.1.i, %x\n  %add.2.i = add i64 %mul.2.i11, %rem.2.i\n  %sext.i = shl i64 %add.2.i, 32\n  %conv6.i = ashr i64 %sext.i, 32\n  %gep = getelementptr i64, ptr %dst, i64 %conv6.i\n  store i64 %div.i, ptr %gep, align 4\n  %iv.next = add i64 %iv, 1\n  %exitcond.not = icmp eq i64 %iv, %N\n  br i1 %exitcond.not, label %exit, label %loop\n\nexit:                                             ; preds = %loop\n  ret void\n}\n"
        },
        {
          "test_name": "sdiv_feeding_gep_predicated",
          "test_body": "define void @sdiv_feeding_gep_predicated(ptr %dst, i32 %x, i64 %M, i64 %conv6, i64 %N) {\nentry:\n  %conv61 = zext i32 %x to i64\n  br label %loop\n\nloop:                                             ; preds = %loop.latch, %entry\n  %iv = phi i64 [ 0, %entry ], [ %iv.next, %loop.latch ]\n  %c = icmp ule i64 %iv, %M\n  br i1 %c, label %then, label %loop.latch\n\nthen:                                             ; preds = %loop\n  %div18 = sdiv i64 %M, %conv6\n  %conv20 = trunc i64 %div18 to i32\n  %mul30 = mul i64 %div18, %conv61\n  %sub31 = sub i64 %iv, %mul30\n  %conv34 = trunc i64 %sub31 to i32\n  %mul35 = mul i32 %x, %conv20\n  %add36 = add i32 %mul35, %conv34\n  %idxprom = sext i32 %add36 to i64\n  %gep = getelementptr double, ptr %dst, i64 %idxprom\n  store double 0.000000e+00, ptr %gep, align 8\n  br label %loop.latch\n\nloop.latch:                                       ; preds = %then, %loop\n  %iv.next = add i64 %iv, 1\n  %ec = icmp eq i64 %iv.next, %N\n  br i1 %ec, label %exit, label %loop\n\nexit:                                             ; preds = %loop.latch\n  ret void\n}\n"
        },
        {
          "test_name": "sdiv_feeding_gep",
          "test_body": "define void @sdiv_feeding_gep(ptr %dst, i32 %x, i64 %M, i64 %conv6, i64 %N) {\nentry:\n  %conv61 = zext i32 %x to i64\n  br label %loop\n\nloop:                                             ; preds = %loop, %entry\n  %iv = phi i64 [ 0, %entry ], [ %iv.next, %loop ]\n  %div18 = sdiv i64 %M, %conv6\n  %conv20 = trunc i64 %div18 to i32\n  %mul30 = mul i64 %div18, %conv61\n  %sub31 = sub i64 %iv, %mul30\n  %conv34 = trunc i64 %sub31 to i32\n  %mul35 = mul i32 %x, %conv20\n  %add36 = add i32 %mul35, %conv34\n  %idxprom = sext i32 %add36 to i64\n  %gep = getelementptr double, ptr %dst, i64 %idxprom\n  store double 0.000000e+00, ptr %gep, align 8\n  %iv.next = add i64 %iv, 1\n  %ec = icmp eq i64 %iv.next, %N\n  br i1 %ec, label %exit, label %loop\n\nexit:                                             ; preds = %loop\n  ret void\n}\n"
        }
      ]
    },
    {
      "file": "llvm/test/Transforms/LoopVectorize/X86/consecutive-ptr-uniforms.ll",
      "commands": [
        "opt < %s -aa-pipeline=basic-aa -passes=loop-vectorize,instcombine -S -debug-only=loop-vectorize -disable-output -print-after=instcombine 2>&1",
        "opt < %s -passes=loop-vectorize -force-vector-width=2 -S"
      ],
      "tests": [
        {
          "test_name": "PR31671",
          "test_body": "target datalayout = \"e-m:e-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-unknown-linux-gnu\"\n\n%data = type { [32000 x float], [3 x i32], [4 x i8], [32000 x float] }\n\ndefine void @PR31671(float %x, ptr %d) #0 {\nentry:\n  br label %for.body\n\nfor.body:                                         ; preds = %for.body, %entry\n  %i = phi i64 [ %i.next, %for.body ], [ 0, %entry ]\n  %tmp0 = getelementptr inbounds %data, ptr %d, i64 0, i32 3, i64 %i\n  %tmp1 = load float, ptr %tmp0, align 4\n  %tmp2 = fmul float %x, %tmp1\n  %tmp3 = getelementptr inbounds %data, ptr %d, i64 0, i32 0, i64 %i\n  %tmp4 = load float, ptr %tmp3, align 4\n  %tmp5 = fadd float %tmp4, %tmp2\n  store float %tmp5, ptr %tmp3, align 4\n  %i.next = add nuw nsw i64 %i, 5\n  %cond = icmp slt i64 %i.next, 32000\n  br i1 %cond, label %for.body, label %for.end\n\nfor.end:                                          ; preds = %for.body\n  ret void\n}\n\nattributes #0 = { \"target-cpu\"=\"knl\" }\n"
        },
        {
          "test_name": "PR40816",
          "test_body": "target datalayout = \"e-m:e-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-unknown-linux-gnu\"\n\n@a = external hidden constant [3 x i32], align 1\n@b = external global i32, align 1\n\ndefine void @PR40816() #0 {\nentry:\n  br label %for.body\n\nfor.body:                                         ; preds = %for.body, %entry\n  %0 = phi i32 [ 0, %entry ], [ %inc, %for.body ]\n  store i32 %0, ptr @b, align 1\n  %arrayidx1 = getelementptr inbounds [3 x i32], ptr @a, i32 0, i32 %0\n  %1 = load i32, ptr %arrayidx1, align 1\n  %cmp2 = icmp eq i32 %1, 0\n  %inc = add nuw nsw i32 %0, 1\n  br i1 %cmp2, label %return, label %for.body\n\nreturn:                                           ; preds = %for.body\n  ret void\n}\n\nattributes #0 = { \"target-cpu\"=\"core2\" }\n"
        }
      ]
    }
  ],
  "issue": {
    "title": "[VPlan] Report \"Assertion `!State->VF.isScalable() && \"VF is assumed to be non scalable.\"' failed\"",
    "body": "The IR is put at the end.\r\n\r\nCompile command is `opt -passes=loop-vectorize -prefer-predicate-over-epilogue=predicate-else-scalar-epilogue`\r\n\r\nThe error is\r\n```\r\nopt: /root/llvm-project/llvm/lib/Transforms/Vectorize/VPlan.cpp:734: virtual void llvm::VPRegionBlock::execute(llvm::VPTransformState*): Assertion `!State->VF.isScalable() && \"VF is assumed to be non scalable.\"' failed.\r\n```\r\n\r\nIt can be seen at  https://godbolt.org/z/s4bqzdKPP\r\n\r\n```\r\n; ModuleID = 'test.cpp'\r\nsource_filename = \"test.cpp\"\r\ntarget datalayout = \"e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128\"\r\ntarget triple = \"aarch64-unknown-linux-gnu\"\r\n\r\n%struct.ident_t = type { i32, i32, i32, i32, ptr }\r\n\r\n@0 = private unnamed_addr constant [23 x i8] c\";unknown;unknown;0;0;;\\00\", align 1\r\n@1 = private unnamed_addr constant %struct.ident_t { i32 0, i32 514, i32 0, i32 22, ptr @0 }, align 8\r\n@2 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2, i32 0, i32 22, ptr @0 }, align 8\r\n\r\n; Function Attrs: mustprogress nounwind uwtable vscale_range(1,16)\r\ndefine dso_local void @_Z4testiiPdS_(i32 noundef %nx, i32 noundef %ik, ptr noundef %out, ptr noundef %rspace) local_unnamed_addr #0 {\r\nentry:\r\n  %nx.addr = alloca i32, align 4\r\n  %ik.addr = alloca i32, align 4\r\n  %out.addr = alloca ptr, align 8\r\n  %rspace.addr = alloca ptr, align 8\r\n  store i32 %nx, ptr %nx.addr, align 4\r\n  store i32 %ik, ptr %ik.addr, align 4\r\n  store ptr %out, ptr %out.addr, align 8\r\n  store ptr %rspace, ptr %rspace.addr, align 8\r\n  call void (ptr, i32, ptr, ...) @__kmpc_fork_call(ptr nonnull @2, i32 4, ptr nonnull @_Z4testiiPdS_.omp_outlined, ptr nonnull %nx.addr, ptr nonnull %ik.addr, ptr nonnull %out.addr, ptr nonnull %rspace.addr)\r\n  ret void\r\n}\r\n\r\n; Function Attrs: alwaysinline norecurse nounwind uwtable vscale_range(1,16)\r\ndefine internal void @_Z4testiiPdS_.omp_outlined(ptr noalias nocapture noundef readonly %.global_tid., ptr noalias nocapture noundef readnone %.bound_tid., ptr noalias nocapture noundef nonnull readonly align 4 dereferenceable(4) %nx, ptr noalias nocapture noundef nonnull readonly align 4 dereferenceable(4) %ik, ptr noalias nocapture noundef nonnull readonly align 8 dereferenceable(8) %out, ptr noalias nocapture noundef nonnull readonly align 8 dereferenceable(8) %rspace) #1 {\r\nentry:\r\n  %.omp.lb = alloca i64, align 8\r\n  %.omp.ub = alloca i64, align 8\r\n  %.omp.stride = alloca i64, align 8\r\n  %.omp.is_last = alloca i32, align 4\r\n  %0 = load i32, ptr %nx, align 4\r\n  %1 = load i32, ptr %ik, align 4\r\n  %cmp = icmp sgt i32 %0, 0\r\n  %cmp8 = icmp sgt i32 %1, 0\r\n  %or.cond = select i1 %cmp, i1 %cmp8, i1 false\r\n  br i1 %or.cond, label %omp.precond.then, label %omp.precond.end\r\n\r\nomp.precond.then:                                 ; preds = %entry\r\n  %conv = zext i32 %0 to i64\r\n  %conv6 = zext i32 %1 to i64\r\n  %mul = mul nuw nsw i64 %conv6, %conv\r\n  %sub7 = add nsw i64 %mul, -1\r\n  call void @llvm.lifetime.start.p0(i64 8, ptr nonnull %.omp.lb) #3\r\n  store i64 0, ptr %.omp.lb, align 8\r\n  call void @llvm.lifetime.start.p0(i64 8, ptr nonnull %.omp.ub) #3\r\n  store i64 %sub7, ptr %.omp.ub, align 8\r\n  call void @llvm.lifetime.start.p0(i64 8, ptr nonnull %.omp.stride) #3\r\n  store i64 1, ptr %.omp.stride, align 8\r\n  call void @llvm.lifetime.start.p0(i64 4, ptr nonnull %.omp.is_last) #3\r\n  store i32 0, ptr %.omp.is_last, align 4\r\n  %2 = load i32, ptr %.global_tid., align 4\r\n  call void @__kmpc_for_static_init_8(ptr nonnull @1, i32 %2, i32 33, ptr nonnull %.omp.is_last, ptr nonnull %.omp.lb, ptr nonnull %.omp.ub, ptr nonnull %.omp.stride, i64 1, i64 512)\r\n  %3 = load i64, ptr %.omp.ub, align 8\r\n  %cond60 = call i64 @llvm.smin.i64(i64 %3, i64 %sub7)\r\n  store i64 %cond60, ptr %.omp.ub, align 8\r\n  %4 = load i64, ptr %.omp.lb, align 8\r\n  %cmp12.not61 = icmp sgt i64 %4, %cond60\r\n  br i1 %cmp12.not61, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph\r\n\r\nomp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then\r\n  br label %omp.inner.for.cond.preheader\r\n\r\nomp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc\r\n  %5 = phi i64 [ %4, %omp.inner.for.cond.preheader.lr.ph ], [ %add42, %omp.dispatch.inc ]\r\n  %cond62 = phi i64 [ %cond60, %omp.inner.for.cond.preheader.lr.ph ], [ %cond, %omp.dispatch.inc ]\r\n  %smax = call i64 @llvm.smax.i64(i64 %cond62, i64 %5)\r\n  %6 = add i64 %smax, 1\r\n  %7 = load ptr, ptr %rspace, align 8\r\n  %8 = load ptr, ptr %out, align 8\r\n  br label %omp.inner.for.body\r\n\r\nomp.inner.for.body:                               ; preds = %omp.inner.for.cond.preheader, %omp.inner.for.body\r\n  %.omp.iv.059 = phi i64 [ %5, %omp.inner.for.cond.preheader ], [ %add41, %omp.inner.for.body ]\r\n  %div18 = sdiv i64 %.omp.iv.059, %conv6\r\n  %conv20 = trunc i64 %div18 to i32\r\n  %mul30 = mul nsw i64 %div18, %conv6\r\n  %sub31 = sub nsw i64 %.omp.iv.059, %mul30\r\n  %conv34 = trunc i64 %sub31 to i32\r\n  %mul35 = mul nsw i32 %1, %conv20\r\n  %add36 = add nsw i32 %mul35, %conv34\r\n  %idxprom = sext i32 %add36 to i64\r\n  %arrayidx = getelementptr inbounds double, ptr %7, i64 %idxprom\r\n  %9 = load double, ptr %arrayidx, align 8\r\n  %arrayidx40 = getelementptr inbounds double, ptr %8, i64 %idxprom\r\n  store double %9, ptr %arrayidx40, align 8\r\n  %add41 = add i64 %.omp.iv.059, 1\r\n  %exitcond = icmp ne i64 %add41, %6\r\n  br i1 %exitcond, label %omp.inner.for.body, label %omp.dispatch.inc\r\n\r\nomp.dispatch.inc:                                 ; preds = %omp.inner.for.body\r\n  %10 = load i64, ptr %.omp.stride, align 8\r\n  %add42 = add nsw i64 %10, %5\r\n  store i64 %add42, ptr %.omp.lb, align 8\r\n  %add43 = add nsw i64 %10, %cond62\r\n  %cond = call i64 @llvm.smin.i64(i64 %add43, i64 %sub7)\r\n  store i64 %cond, ptr %.omp.ub, align 8\r\n  %cmp12.not = icmp sgt i64 %add42, %cond\r\n  br i1 %cmp12.not, label %omp.dispatch.cond.omp.dispatch.end_crit_edge, label %omp.inner.for.cond.preheader\r\n\r\nomp.dispatch.cond.omp.dispatch.end_crit_edge:     ; preds = %omp.dispatch.inc\r\n  br label %omp.dispatch.end\r\n\r\nomp.dispatch.end:                                 ; preds = %omp.dispatch.cond.omp.dispatch.end_crit_edge, %omp.precond.then\r\n  call void @__kmpc_for_static_fini(ptr nonnull @1, i32 %2)\r\n  call void @llvm.lifetime.end.p0(i64 4, ptr nonnull %.omp.is_last) #3\r\n  call void @llvm.lifetime.end.p0(i64 8, ptr nonnull %.omp.stride) #3\r\n  call void @llvm.lifetime.end.p0(i64 8, ptr nonnull %.omp.ub) #3\r\n  call void @llvm.lifetime.end.p0(i64 8, ptr nonnull %.omp.lb) #3\r\n  br label %omp.precond.end\r\n\r\nomp.precond.end:                                  ; preds = %omp.dispatch.end, %entry\r\n  ret void\r\n}\r\n\r\n; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn memory(argmem: readwrite)\r\ndeclare void @llvm.lifetime.start.p0(i64 immarg, ptr nocapture) #2\r\n\r\n; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn memory(argmem: readwrite)\r\ndeclare void @llvm.lifetime.end.p0(i64 immarg, ptr nocapture) #2\r\n\r\n; Function Attrs: nounwind\r\ndeclare void @__kmpc_for_static_init_8(ptr, i32, i32, ptr, ptr, ptr, ptr, i64, i64) local_unnamed_addr #3\r\n\r\n; Function Attrs: nounwind\r\ndeclare void @__kmpc_for_static_fini(ptr, i32) local_unnamed_addr #3\r\n\r\n; Function Attrs: nounwind\r\ndeclare void @__kmpc_fork_call(ptr, i32, ptr, ...) local_unnamed_addr #3\r\n\r\n; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)\r\ndeclare i64 @llvm.smin.i64(i64, i64) #4\r\n\r\n; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)\r\ndeclare i64 @llvm.smax.i64(i64, i64) #4\r\n\r\nattributes #0 = { mustprogress nounwind uwtable vscale_range(1,16) \"frame-pointer\"=\"non-leaf\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"generic\" \"target-features\"=\"+fp-armv8,+fullfp16,+neon,+sve,+v8a,-fmv\" }\r\nattributes #1 = { alwaysinline norecurse nounwind uwtable vscale_range(1,16) \"frame-pointer\"=\"non-leaf\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"generic\" \"target-features\"=\"+fp-armv8,+fullfp16,+neon,+sve,+v8a,-fmv\" }\r\nattributes #2 = { mustprogress nocallback nofree nosync nounwind willreturn memory(argmem: readwrite) }\r\nattributes #3 = { nounwind }\r\nattributes #4 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }\r\n\r\n```",
    "author": "eastB233",
    "labels": [
      "vectorizers",
      "crash"
    ],
    "comments": [
      {
        "author": "eastB233",
        "body": "Simpler test case is\r\n```c\r\nextern const int npy;\r\nextern const int nx;\r\ndouble* rspace;\r\n\r\nvoid recip2real(double* out, const double factor)\r\n{\r\n#ifdef _OPENMP\r\n#pragma omp parallel for collapse(2)\r\n#endif\r\n        for (int ix = 0; ix < nx; ++ix)\r\n        {\r\n            for (int ipy = 0; ipy < npy; ++ipy) {\r\n                out[ix * npy + ipy] += factor * rspace[ix * npy + ipy];\r\n            }\r\n        }\r\n}\r\n```\r\n\r\nCommand is `clang -O3 -march=armv9-a -fopenmp -mllvm -prefer-predicate-over-epilogue=predicate-else-scalar-epilogue -S`\r\n\r\nIt can be seen at   https://godbolt.org/z/6MGsecEr5"
      },
      {
        "author": "eastB233",
        "body": "I don't know much about VPlan, and as far as I understand by tracing the code,\r\n\r\n### first, it fails at the following VPlan\r\n```\r\nVPlan 'Initial VPlan for VF={vscale x 1,vscale x 2},UF>=1' {\r\nvp<%2> = original trip-count\r\n\r\nph:\r\n  EMIT vp<%2> = EXPAND SCEV (1 + (-1 * %5) + ((-1 + ((zext i32 %0 to i64) * (sext i32 %1 to i64)))<nsw> smin %4))\r\nNo successors\r\n\r\nvector.ph:\r\n  EMIT vp<%3> = TC > VF ? TC - VF : 0 vp<%2>\r\n  EMIT vp<%4> = VF * Part + ir<0>\r\n  EMIT vp<%5> = active lane mask vp<%4>, vp<%2>\r\nSuccessor(s): vector loop\r\n\r\n<x1> vector loop: {\r\n  vector.body:\r\n    EMIT vp<%6> = CANONICAL-INDUCTION\r\n    ACTIVE-LANE-MASK-PHI vp<%7> = phi vp<%5>, vp<%27>\r\n    vp<%8>    = DERIVED-IV ir<%5> + vp<%6> * ir<1>\r\n    vp<%9>    = SCALAR-STEPS vp<%8>, ir<1>\r\n  Successor(s): pred.sdiv\r\n\r\n  <xVFxUF> pred.sdiv: {\r\n    pred.sdiv.entry:\r\n      BRANCH-ON-MASK vp<%7>\r\n    Successor(s): pred.sdiv.if, pred.sdiv.continue\r\n\r\n    pred.sdiv.if:\r\n      CLONE ir<%div24> = sdiv vp<%9>, ir<%conv6>\r\n    Successor(s): pred.sdiv.continue\r\n\r\n    pred.sdiv.continue:\r\n      PHI-PREDICATED-INSTRUCTION vp<%11> = ir<%div24>\r\n    No successors\r\n  }\r\n  Successor(s): omp.inner.for.body.0\r\n\r\n  omp.inner.for.body.0:\r\n    CLONE ir<%conv26> = trunc vp<%11>\r\n    CLONE ir<%mul36> = mul nsw vp<%11>, ir<%conv6>\r\n    CLONE ir<%sub37> = sub nsw vp<%9>, ir<%mul36>\r\n    CLONE ir<%conv40> = trunc ir<%sub37>\r\n    CLONE ir<%mul41> = mul nsw ir<%1>, ir<%conv26>\r\n    CLONE ir<%add42> = add nsw ir<%mul41>, ir<%conv40>\r\n    CLONE ir<%idxprom> = sext ir<%add42>\r\n    CLONE ir<%arrayidx> = getelementptr inbounds ir<%6>, ir<%idxprom>\r\n    WIDEN ir<%8> = load ir<%arrayidx>, vp<%7>\r\n    WIDEN ir<%mul43> = fmul contract ir<%2>, ir<%8>\r\n    CLONE ir<%arrayidx47> = getelementptr inbounds ir<%7>, ir<%idxprom>\r\n    WIDEN ir<%9> = load ir<%arrayidx47>, vp<%7>\r\n    WIDEN ir<%add48> = fadd contract ir<%mul43>, ir<%9>\r\n    WIDEN store ir<%arrayidx47>, ir<%add48>, vp<%7>\r\n    EMIT vp<%25> = VF * UF + vp<%6>\r\n    EMIT vp<%26> = VF * Part + vp<%6>\r\n    EMIT vp<%27> = active lane mask vp<%26>, vp<%3>\r\n    EMIT vp<%28> = not vp<%27>\r\n    EMIT branch-on-cond vp<%28>\r\n  No successors\r\n}\r\nSuccessor(s): middle.block\r\n```\r\n\r\nVPRegion `pred.sdiv` fails at assertion\r\n```c++\r\nvoid VPRegionBlock::execute(VPTransformState *State) {\r\n...\r\n  if (!isReplicator()) {\r\n...\r\n    return;\r\n  }\r\n...\r\n  for (...) {\r\n    assert(!State->VF.isScalable() && \"VF is assumed to be non scalable.\");\r\n  }\r\n}\r\n```\r\nI think VPRegion `pred.sdiv` should have `isReplicator() == false` or `pred.sdiv` just should not exist.\r\n\r\n### second, I find VPRegion `pred.sdiv` is splitted from `WorkList`\r\n```c++\r\nstatic void addReplicateRegions(VPlan &Plan) {\r\n  SmallVector<VPReplicateRecipe *> WorkList;\r\n  for (VPBasicBlock *VPBB : VPBlockUtils::blocksOnly<VPBasicBlock>(\r\n           vp_depth_first_deep(Plan.getEntry()))) {\r\n    for (VPRecipeBase &R : *VPBB)\r\n      if (auto *RepR = dyn_cast<VPReplicateRecipe>(&R)) {\r\n        if (RepR->isPredicated())\r\n          WorkList.push_back(RepR);\r\n      }\r\n  }\r\n...\r\n}\r\n```\r\nby VPRecipe `CLONE ir<%div24> = sdiv ir<%.omp.iv.065>, ir<%conv6>, vp<%7>`, where the instruction is `%div24 = sdiv i64 %.omp.iv.065, %conv6`\r\nI think this VPRecipe should have `isPredicated() == false` here, so it will not be splitted.\r\n\r\n### third, I find this VPRecipe is created here\r\n```c++\r\nVPRecipeOrVPValueTy VPRecipeBuilder::handleReplication(...) {\r\n  bool IsUniform = LoopVectorizationPlanner::getDecisionAndClampRange(\r\n      [&](ElementCount VF) { return CM.isUniformAfterVectorization(I, VF); },\r\n      Range);\r\n\r\n  bool IsPredicated = CM.isPredicatedInst(I);\r\n...\r\n  VPValue *BlockInMask = nullptr;\r\n  if (!IsPredicated) {\r\n    // Finalize the recipe for Instr, first if it is not predicated.\r\n    LLVM_DEBUG(dbgs() << \"LV: Scalarizing:\" << *I << \"\\n\");\r\n  } else {\r\n    LLVM_DEBUG(dbgs() << \"LV: Scalarizing and predicating:\" << *I << \"\\n\");\r\n    // Instructions marked for predication are replicated and a mask operand is\r\n    // added initially. Masked replicate recipes will later be placed under an\r\n    // if-then construct to prevent side-effects. Generate recipes to compute\r\n    // the block mask for this region.\r\n    BlockInMask = createBlockInMask(I->getParent(), Plan);\r\n  }\r\n\r\n  auto *Recipe = new VPReplicateRecipe(I, Plan.mapToVPValues(I->operands()),\r\n                                       IsUniform, BlockInMask);\r\n  return toVPRecipeResult(Recipe);\r\n}\r\n```\r\nI notice that instruction `I` (`%div24 = sdiv i64 %.omp.iv.065, %conv6`) do not need to vectorize, because function `isScalarAfterVectorization` returns true and `I` is just used to calculate the index.\r\nIt seems reasonable that a `scalar` instruction does not need `Predicated`.\r\n\r\n### So I try the following modification,\r\n```diff\r\ndiff --git a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp\r\nindex 907b8ce002e8..76a5704a61c5 100644\r\n--- a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp\r\n+++ b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp\r\n@@ -9819,7 +9819,9 @@ VPRecipeOrVPValueTy VPRecipeBuilder::handleReplication(Instruction *I,\r\n       [&](ElementCount VF) { return CM.isUniformAfterVectorization(I, VF); },\r\n       Range);\r\n\r\n-  bool IsPredicated = CM.isPredicatedInst(I);\r\n+  bool IsPredicated = LoopVectorizationPlanner::getDecisionAndClampRange(\r\n+      [&](ElementCount VF) { return CM.isPredicatedInst(I) && !CM.isScalarAfterVectorization(I, VF); },\r\n+      Range);\r\n\r\n   // Even if the instruction is not marked as uniform, there are certain\r\n   // intrinsic calls that can be effectively treated as such, so we check for\r\n```\r\n\r\nJust my guess, I'm not sure if it is correct direction."
      },
      {
        "author": "eastB233",
        "body": "Ping @fhahn "
      },
      {
        "author": "eastB233",
        "body": "I think I misunderstand something, and the changes above may be wrong.\r\n\r\nAnd I have another way. In instruction `%div24 = sdiv i64 %.omp.iv.065, %conv6`, `%conv6` is invariant in loop, so it seems we do not need `Predicated`.\r\n\r\nI try the following patch,\r\n```diff\r\ndiff --git a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp\r\nindex c7c19ef456c7..f294703e1529 100644\r\n--- a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp\r\n+++ b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp\r\n@@ -3856,21 +3856,21 @@ bool LoopVectorizationCostModel::isPredicatedInst(Instruction *I) const {\r\n         !Legal->blockNeedsPredication(I->getParent()))\r\n       return false;\r\n     return true;\r\n   }\r\n   case Instruction::UDiv:\r\n   case Instruction::SDiv:\r\n   case Instruction::SRem:\r\n   case Instruction::URem:\r\n     // TODO: We can use the loop-preheader as context point here and get\r\n     // context sensitive reasoning\r\n-    return !isSafeToSpeculativelyExecute(I);\r\n+    return !isSafeToSpeculativelyExecute(I) && !Legal->isInvariant(I->getOperand(1));\r\n   case Instruction::Call:\r\n     return Legal->isMaskRequired(I);\r\n   }\r\n }\r\n\r\n std::pair<InstructionCost, InstructionCost>\r\n LoopVectorizationCostModel::getDivRemSpeculationCost(Instruction *I,\r\n                                                     ElementCount VF) const {\r\n   assert(I->getOpcode() == Instruction::UDiv ||\r\n          I->getOpcode() == Instruction::SDiv ||\r\n\r\n```"
      },
      {
        "author": "eastB233",
        "body": "ping @sdesmalen-arm @davemgreen @paulwalker-arm "
      },
      {
        "author": "fhahn",
        "body": "@eastB233 unfortunately I don't think this change is correct, e.g. consider https://github.com/llvm/llvm-project/blob/967eba07549d64f15e7a91e798aa46214704f62b/llvm/test/Transforms/LoopVectorize/X86/divs-with-tail-folding.ll#L251 when the sdiv/udiv is executed conditionally in the loop"
      }
    ]
  }
}