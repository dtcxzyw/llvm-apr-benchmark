{
  "bug_id": "139202",
  "issue_url": "https://github.com/llvm/llvm-project/issues/139202",
  "bug_type": "miscompilation",
  "base_commit": "8bd35ca41253ea36fff78d5acf59956a30b6555b",
  "knowledge_cutoff": "2025-05-09T03:49:41Z",
  "lit_test_dir": [
    "llvm/test/Transforms/SLPVectorizer"
  ],
  "hints": {
    "fix_commit": "c807395011a027caae9ac196edfac328fb90443a",
    "components": [
      "LoopAccessAnalysis",
      "SLPVectorizer"
    ],
    "bug_location_lineno": {
      "llvm/include/llvm/Analysis/LoopAccessAnalysis.h": [
        [
          853,
          863
        ]
      ],
      "llvm/lib/Analysis/LoopAccessAnalysis.cpp": [
        [
          1541,
          1551
        ],
        [
          1570,
          1576
        ],
        [
          1585,
          1591
        ],
        [
          1594,
          1603
        ],
        [
          1616,
          1629
        ],
        [
          1654,
          1660
        ]
      ],
      "llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp": [
        [
          1584,
          1590
        ],
        [
          2216,
          2222
        ],
        [
          3619,
          3627
        ],
        [
          5368,
          5374
        ],
        [
          5442,
          5448
        ],
        [
          5540,
          5547
        ],
        [
          5868,
          5874
        ],
        [
          5894,
          5900
        ],
        [
          5921,
          5931
        ],
        [
          6051,
          6059
        ],
        [
          6061,
          6069
        ],
        [
          6081,
          6089
        ],
        [
          6122,
          6128
        ],
        [
          6165,
          6174
        ],
        [
          6427,
          6434
        ],
        [
          6441,
          6450
        ],
        [
          6494,
          6503
        ],
        [
          7007,
          7013
        ],
        [
          7999,
          8005
        ],
        [
          8027,
          8040
        ],
        [
          8046,
          8052
        ],
        [
          8130,
          8144
        ],
        [
          8154,
          8160
        ],
        [
          8176,
          8186
        ],
        [
          8191,
          8206
        ],
        [
          8231,
          8240
        ],
        [
          8249,
          8255
        ],
        [
          8259,
          8265
        ],
        [
          8267,
          8273
        ],
        [
          8280,
          8288
        ],
        [
          8291,
          8298
        ],
        [
          8454,
          8481
        ],
        [
          8698,
          8709
        ],
        [
          9299,
          9308
        ],
        [
          10751,
          10757
        ],
        [
          11950,
          11956
        ],
        [
          18270,
          18281
        ],
        [
          21133,
          21150
        ],
        [
          21155,
          21161
        ],
        [
          21171,
          21177
        ],
        [
          21204,
          21210
        ],
        [
          21505,
          21511
        ]
      ]
    },
    "bug_location_funcname": {
      "llvm/lib/Analysis/LoopAccessAnalysis.cpp": [
        "llvm::getPointersDiff",
        "llvm::getPtrStride",
        "llvm::isConsecutiveAccess",
        "llvm::sortPtrAccesses"
      ],
      "llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp": [
        "BoUpSLP::canFormVector",
        "BoUpSLP::canMapToVector",
        "BoUpSLP::canVectorizeLoads",
        "BoUpSLP::collectUserStores",
        "BoUpSLP::findReusedOrderedScalars",
        "BoUpSLP::getScalarsVectorizationState",
        "BoUpSLP::transformNodes",
        "BoUpSLP::tryToVectorizeGatheredLoads",
        "BoUpSLP::vectorizeTree",
        "SLPVectorizerPass::vectorizeStores",
        "buildCompressMask",
        "clearVectorizedStores",
        "clusterSortPtrAccesses",
        "combineOrders",
        "fixupOrderingIndices",
        "gatherPossiblyVectorizableLoads",
        "getPointerDiff",
        "getShallowScore",
        "getStores",
        "insertOrLookup",
        "isMaskedLoadCompress",
        "isStridedLoad",
        "rebase"
      ]
    }
  },
  "patch": "commit c807395011a027caae9ac196edfac328fb90443a\nAuthor: Ramkumar Ramachandra <ramkumar.ramachandra@codasip.com>\nDate:   Thu May 15 10:08:05 2025 +0100\n\n    [LAA/SLP] Don't truncate APInt in getPointersDiff (#139941)\n    \n    Change getPointersDiff to return an std::optional<int64_t>, and fill\n    this value with using APInt::trySExtValue. This simple change requires\n    changes to other functions in LAA, and major changes in SLPVectorizer\n    changing types from 32-bit to 64-bit.\n    \n    Fixes #139202.\n\ndiff --git a/llvm/include/llvm/Analysis/LoopAccessAnalysis.h b/llvm/include/llvm/Analysis/LoopAccessAnalysis.h\nindex f715e0ec8dbb..fea2ede8b5ab 100644\n--- a/llvm/include/llvm/Analysis/LoopAccessAnalysis.h\n+++ b/llvm/include/llvm/Analysis/LoopAccessAnalysis.h\n@@ -853,11 +853,10 @@ getPtrStride(PredicatedScalarEvolution &PSE, Type *AccessTy, Value *Ptr,\n /// is a simple API that does not depend on the analysis pass.\n /// \\param StrictCheck Ensure that the calculated distance matches the\n /// type-based one after all the bitcasts removal in the provided pointers.\n-std::optional<int> getPointersDiff(Type *ElemTyA, Value *PtrA, Type *ElemTyB,\n-                                   Value *PtrB, const DataLayout &DL,\n-                                   ScalarEvolution &SE,\n-                                   bool StrictCheck = false,\n-                                   bool CheckType = true);\n+std::optional<int64_t>\n+getPointersDiff(Type *ElemTyA, Value *PtrA, Type *ElemTyB, Value *PtrB,\n+                const DataLayout &DL, ScalarEvolution &SE,\n+                bool StrictCheck = false, bool CheckType = true);\n \n /// Attempt to sort the pointers in \\p VL and return the sorted indices\n /// in \\p SortedIndices, if reordering is required.\ndiff --git a/llvm/lib/Analysis/LoopAccessAnalysis.cpp b/llvm/lib/Analysis/LoopAccessAnalysis.cpp\nindex af1a3c593c51..ab407e945bc5 100644\n--- a/llvm/lib/Analysis/LoopAccessAnalysis.cpp\n+++ b/llvm/lib/Analysis/LoopAccessAnalysis.cpp\n@@ -1541,11 +1541,11 @@ llvm::getPtrStride(PredicatedScalarEvolution &PSE, Type *AccessTy, Value *Ptr,\n   return std::nullopt;\n }\n \n-std::optional<int> llvm::getPointersDiff(Type *ElemTyA, Value *PtrA,\n-                                         Type *ElemTyB, Value *PtrB,\n-                                         const DataLayout &DL,\n-                                         ScalarEvolution &SE, bool StrictCheck,\n-                                         bool CheckType) {\n+std::optional<int64_t> llvm::getPointersDiff(Type *ElemTyA, Value *PtrA,\n+                                             Type *ElemTyB, Value *PtrB,\n+                                             const DataLayout &DL,\n+                                             ScalarEvolution &SE,\n+                                             bool StrictCheck, bool CheckType) {\n   assert(PtrA && PtrB && \"Expected non-nullptr pointers.\");\n \n   // Make sure that A and B are different pointers.\n@@ -1570,7 +1570,7 @@ std::optional<int> llvm::getPointersDiff(Type *ElemTyA, Value *PtrA,\n   const Value *PtrB1 = PtrB->stripAndAccumulateConstantOffsets(\n       DL, OffsetB, /*AllowNonInbounds=*/true);\n \n-  int Val;\n+  std::optional<int64_t> Val;\n   if (PtrA1 == PtrB1) {\n     // Retrieve the address space again as pointer stripping now tracks through\n     // `addrspacecast`.\n@@ -1585,7 +1585,7 @@ std::optional<int> llvm::getPointersDiff(Type *ElemTyA, Value *PtrA,\n     OffsetB = OffsetB.sextOrTrunc(IdxWidth);\n \n     OffsetB -= OffsetA;\n-    Val = OffsetB.getSExtValue();\n+    Val = OffsetB.trySExtValue();\n   } else {\n     // Otherwise compute the distance with SCEV between the base pointers.\n     const SCEV *PtrSCEVA = SE.getSCEV(PtrA);\n@@ -1594,10 +1594,14 @@ std::optional<int> llvm::getPointersDiff(Type *ElemTyA, Value *PtrA,\n         SE.computeConstantDifference(PtrSCEVB, PtrSCEVA);\n     if (!Diff)\n       return std::nullopt;\n-    Val = Diff->getSExtValue();\n+    Val = Diff->trySExtValue();\n   }\n-  int Size = DL.getTypeStoreSize(ElemTyA);\n-  int Dist = Val / Size;\n+\n+  if (!Val)\n+    return std::nullopt;\n+\n+  int64_t Size = DL.getTypeStoreSize(ElemTyA);\n+  int64_t Dist = *Val / Size;\n \n   // Ensure that the calculated distance matches the type-based one after all\n   // the bitcasts removal in the provided pointers.\n@@ -1616,14 +1620,15 @@ bool llvm::sortPtrAccesses(ArrayRef<Value *> VL, Type *ElemTy,\n   // first pointer in the array.\n   Value *Ptr0 = VL[0];\n \n-  using DistOrdPair = std::pair<int64_t, int>;\n+  using DistOrdPair = std::pair<int64_t, unsigned>;\n   auto Compare = llvm::less_first();\n   std::set<DistOrdPair, decltype(Compare)> Offsets(Compare);\n   Offsets.emplace(0, 0);\n   bool IsConsecutive = true;\n   for (auto [Idx, Ptr] : drop_begin(enumerate(VL))) {\n-    std::optional<int> Diff = getPointersDiff(ElemTy, Ptr0, ElemTy, Ptr, DL, SE,\n-                                              /*StrictCheck=*/true);\n+    std::optional<int64_t> Diff =\n+        getPointersDiff(ElemTy, Ptr0, ElemTy, Ptr, DL, SE,\n+                        /*StrictCheck=*/true);\n     if (!Diff)\n       return false;\n \n@@ -1654,7 +1659,7 @@ bool llvm::isConsecutiveAccess(Value *A, Value *B, const DataLayout &DL,\n     return false;\n   Type *ElemTyA = getLoadStoreType(A);\n   Type *ElemTyB = getLoadStoreType(B);\n-  std::optional<int> Diff =\n+  std::optional<int64_t> Diff =\n       getPointersDiff(ElemTyA, PtrA, ElemTyB, PtrB, DL, SE,\n                       /*StrictCheck=*/true, CheckType);\n   return Diff && *Diff == 1;\ndiff --git a/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp b/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp\nindex d8cf6b82a197..eb339282fdae 100644\n--- a/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp\n+++ b/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp\n@@ -1584,7 +1584,7 @@ static void addMask(SmallVectorImpl<int> &Mask, ArrayRef<int> SubMask,\n /// before:  6 9 5 4 9 2 1 0\n /// after:   6 3 5 4 7 2 1 0\n static void fixupOrderingIndices(MutableArrayRef<unsigned> Order) {\n-  const unsigned Sz = Order.size();\n+  const size_t Sz = Order.size();\n   SmallBitVector UnusedIndices(Sz, /*t=*/true);\n   SmallBitVector MaskedIndices(Sz);\n   for (unsigned I = 0; I < Sz; ++I) {\n@@ -2216,7 +2216,7 @@ public:\n             !LI2->isSimple())\n           return CheckSameEntryOrFail();\n \n-        std::optional<int> Dist = getPointersDiff(\n+        std::optional<int64_t> Dist = getPointersDiff(\n             LI1->getType(), LI1->getPointerOperand(), LI2->getType(),\n             LI2->getPointerOperand(), DL, SE, /*StrictCheck=*/true);\n         if (!Dist || *Dist == 0) {\n@@ -3619,9 +3619,10 @@ private:\n   /// vector loads/masked gathers instead of regular gathers. Later these loads\n   /// are reshufled to build final gathered nodes.\n   void tryToVectorizeGatheredLoads(\n-      const SmallMapVector<std::tuple<BasicBlock *, Value *, Type *>,\n-                           SmallVector<SmallVector<std::pair<LoadInst *, int>>>,\n-                           8> &GatheredLoads);\n+      const SmallMapVector<\n+          std::tuple<BasicBlock *, Value *, Type *>,\n+          SmallVector<SmallVector<std::pair<LoadInst *, int64_t>>>, 8>\n+          &GatheredLoads);\n \n   /// Helper for `findExternalStoreUsersReorderIndices()`. It iterates over the\n   /// users of \\p TE and collects the stores. It returns the map from the store\n@@ -5368,7 +5369,7 @@ BoUpSLP::findReusedOrderedScalars(const BoUpSLP::TreeEntry &TE,\n   // patterns.\n   SmallVector<Value *> GatheredScalars(TE.Scalars.begin(), TE.Scalars.end());\n   Type *ScalarTy = GatheredScalars.front()->getType();\n-  int NumScalars = GatheredScalars.size();\n+  size_t NumScalars = GatheredScalars.size();\n   if (!isValidElementType(ScalarTy))\n     return std::nullopt;\n   auto *VecTy = getWidenedType(ScalarTy, NumScalars);\n@@ -5442,7 +5443,7 @@ BoUpSLP::findReusedOrderedScalars(const BoUpSLP::TreeEntry &TE,\n       unsigned Limit = getNumElems(CurrentOrder.size(), PartSz, I);\n       MutableArrayRef<unsigned> Slice = CurrentOrder.slice(I * PartSz, Limit);\n       // Shuffle of at least 2 vectors - ignore.\n-      if (any_of(Slice, [&](int I) { return I != NumScalars; })) {\n+      if (any_of(Slice, [&](unsigned I) { return I != NumScalars; })) {\n         std::fill(Slice.begin(), Slice.end(), NumScalars);\n         ShuffledSubMasks.set(I);\n         continue;\n@@ -5540,8 +5541,8 @@ BoUpSLP::findReusedOrderedScalars(const BoUpSLP::TreeEntry &TE,\n       return std::max(Entries[I].front()->getVectorFactor(),\n                       Entries[I].back()->getVectorFactor());\n     });\n-  int NumUndefs =\n-      count_if(CurrentOrder, [&](int Idx) { return Idx == NumScalars; });\n+  unsigned NumUndefs =\n+      count_if(CurrentOrder, [&](unsigned Idx) { return Idx == NumScalars; });\n   if (ShuffledSubMasks.all() || (NumScalars > 2 && NumUndefs >= NumScalars / 2))\n     return std::nullopt;\n   return std::move(CurrentOrder);\n@@ -5868,7 +5869,11 @@ static bool buildCompressMask(ArrayRef<Value *> PointerOps,\n   Value *Ptr0 = Order.empty() ? PointerOps.front() : PointerOps[Order.front()];\n   for (unsigned I : seq<unsigned>(1, Sz)) {\n     Value *Ptr = Order.empty() ? PointerOps[I] : PointerOps[Order[I]];\n-    unsigned Pos = *getPointersDiff(ScalarTy, Ptr0, ScalarTy, Ptr, DL, SE);\n+    std::optional<int64_t> OptPos =\n+        getPointersDiff(ScalarTy, Ptr0, ScalarTy, Ptr, DL, SE);\n+    if (!OptPos || OptPos > std::numeric_limits<unsigned>::max())\n+      return false;\n+    unsigned Pos = static_cast<unsigned>(*OptPos);\n     CompressMask[I] = Pos;\n     if (!Stride)\n       continue;\n@@ -5894,7 +5899,7 @@ static bool isMaskedLoadCompress(\n     VectorType *&LoadVecTy) {\n   InterleaveFactor = 0;\n   Type *ScalarTy = VL.front()->getType();\n-  const unsigned Sz = VL.size();\n+  const size_t Sz = VL.size();\n   auto *VecTy = getWidenedType(ScalarTy, Sz);\n   constexpr TTI::TargetCostKind CostKind = TTI::TCK_RecipThroughput;\n   SmallVector<int> Mask;\n@@ -5921,11 +5926,11 @@ static bool isMaskedLoadCompress(\n     Ptr0 = PointerOps[Order.front()];\n     PtrN = PointerOps[Order.back()];\n   }\n-  std::optional<int> Diff =\n+  std::optional<int64_t> Diff =\n       getPointersDiff(ScalarTy, Ptr0, ScalarTy, PtrN, DL, SE);\n   if (!Diff)\n     return false;\n-  const unsigned MaxRegSize =\n+  const size_t MaxRegSize =\n       TTI.getRegisterBitWidth(TargetTransformInfo::RGK_FixedWidthVector)\n           .getFixedValue();\n   // Check for very large distances between elements.\n@@ -6051,9 +6056,10 @@ static bool isStridedLoad(ArrayRef<Value *> VL, ArrayRef<Value *> PointerOps,\n                           ArrayRef<unsigned> Order,\n                           const TargetTransformInfo &TTI, const DataLayout &DL,\n                           ScalarEvolution &SE,\n-                          const bool IsAnyPointerUsedOutGraph, const int Diff) {\n-  const unsigned Sz = VL.size();\n-  const unsigned AbsoluteDiff = std::abs(Diff);\n+                          const bool IsAnyPointerUsedOutGraph,\n+                          const int64_t Diff) {\n+  const size_t Sz = VL.size();\n+  const uint64_t AbsoluteDiff = std::abs(Diff);\n   Type *ScalarTy = VL.front()->getType();\n   auto *VecTy = getWidenedType(ScalarTy, Sz);\n   if (IsAnyPointerUsedOutGraph ||\n@@ -6061,9 +6067,9 @@ static bool isStridedLoad(ArrayRef<Value *> VL, ArrayRef<Value *> PointerOps,\n        (Sz > MinProfitableStridedLoads ||\n         (AbsoluteDiff <= MaxProfitableLoadStride * Sz &&\n          AbsoluteDiff % Sz == 0 && has_single_bit(AbsoluteDiff / Sz)))) ||\n-      Diff == -(static_cast<int>(Sz) - 1)) {\n-    int Stride = Diff / static_cast<int>(Sz - 1);\n-    if (Diff != Stride * static_cast<int>(Sz - 1))\n+      Diff == -(static_cast<int64_t>(Sz) - 1)) {\n+    int64_t Stride = Diff / static_cast<int64_t>(Sz - 1);\n+    if (Diff != Stride * static_cast<int64_t>(Sz - 1))\n       return false;\n     Align Alignment =\n         cast<LoadInst>(Order.empty() ? VL.front() : VL[Order.front()])\n@@ -6081,9 +6087,9 @@ static bool isStridedLoad(ArrayRef<Value *> VL, ArrayRef<Value *> PointerOps,\n     }\n     // Iterate through all pointers and check if all distances are\n     // unique multiple of Dist.\n-    SmallSet<int, 4> Dists;\n+    SmallSet<int64_t, 4> Dists;\n     for (Value *Ptr : PointerOps) {\n-      int Dist = 0;\n+      int64_t Dist = 0;\n       if (Ptr == PtrN)\n         Dist = Diff;\n       else if (Ptr != Ptr0)\n@@ -6122,7 +6128,7 @@ BoUpSLP::canVectorizeLoads(ArrayRef<Value *> VL, const Value *VL0,\n   // Make sure all loads in the bundle are simple - we can't vectorize\n   // atomic or volatile loads.\n   PointerOps.clear();\n-  const unsigned Sz = VL.size();\n+  const size_t Sz = VL.size();\n   PointerOps.resize(Sz);\n   auto *POIter = PointerOps.begin();\n   for (Value *V : VL) {\n@@ -6165,10 +6171,10 @@ BoUpSLP::canVectorizeLoads(ArrayRef<Value *> VL, const Value *VL0,\n       Ptr0 = PointerOps[Order.front()];\n       PtrN = PointerOps[Order.back()];\n     }\n-    std::optional<int> Diff =\n+    std::optional<int64_t> Diff =\n         getPointersDiff(ScalarTy, Ptr0, ScalarTy, PtrN, *DL, *SE);\n     // Check that the sorted loads are consecutive.\n-    if (static_cast<unsigned>(*Diff) == Sz - 1)\n+    if (static_cast<uint64_t>(*Diff) == Sz - 1)\n       return LoadsState::Vectorize;\n     if (isMaskedLoadCompress(VL, PointerOps, Order, *TTI, *DL, *SE, *AC, *DT,\n                              *TLI, [&](Value *V) {\n@@ -6427,8 +6433,9 @@ static bool clusterSortPtrAccesses(ArrayRef<Value *> VL,\n   // Map from bases to a vector of (Ptr, Offset, OrigIdx), which we insert each\n   // Ptr into, sort and return the sorted indices with values next to one\n   // another.\n-  SmallMapVector<std::pair<BasicBlock *, Value *>,\n-                 SmallVector<SmallVector<std::tuple<Value *, int, unsigned>>>, 8>\n+  SmallMapVector<\n+      std::pair<BasicBlock *, Value *>,\n+      SmallVector<SmallVector<std::tuple<Value *, int64_t, unsigned>>>, 8>\n       Bases;\n   Bases\n       .try_emplace(std::make_pair(\n@@ -6441,10 +6448,10 @@ static bool clusterSortPtrAccesses(ArrayRef<Value *> VL,\n                               getUnderlyingObject(Ptr, RecursionMaxDepth));\n     bool Found = any_of(Bases.try_emplace(Key).first->second,\n                         [&, &Cnt = Cnt, &Ptr = Ptr](auto &Base) {\n-                          std::optional<int> Diff = getPointersDiff(\n-                              ElemTy, std::get<0>(Base.front()), ElemTy,\n-                              Ptr, DL, SE,\n-                              /*StrictCheck=*/true);\n+                          std::optional<int64_t> Diff =\n+                              getPointersDiff(ElemTy, std::get<0>(Base.front()),\n+                                              ElemTy, Ptr, DL, SE,\n+                                              /*StrictCheck=*/true);\n                           if (!Diff)\n                             return false;\n \n@@ -6494,10 +6501,11 @@ static bool clusterSortPtrAccesses(ArrayRef<Value *> VL,\n     for (auto &Vec : Base.second) {\n       if (Vec.size() > 1) {\n         stable_sort(Vec, llvm::less_second());\n-        int InitialOffset = std::get<1>(Vec[0]);\n+        int64_t InitialOffset = std::get<1>(Vec[0]);\n         bool AnyConsecutive =\n             all_of(enumerate(Vec), [InitialOffset](const auto &P) {\n-              return std::get<1>(P.value()) == int(P.index()) + InitialOffset;\n+              return std::get<1>(P.value()) ==\n+                     int64_t(P.index()) + InitialOffset;\n             });\n         // Fill SortedIndices array only if it looks worth-while to sort the\n         // ptrs.\n@@ -7007,7 +7015,7 @@ static void combineOrders(MutableArrayRef<unsigned> Order,\n                           ArrayRef<unsigned> SecondaryOrder) {\n   assert((SecondaryOrder.empty() || Order.size() == SecondaryOrder.size()) &&\n          \"Expected same size of orders\");\n-  unsigned Sz = Order.size();\n+  size_t Sz = Order.size();\n   SmallBitVector UsedIndices(Sz);\n   for (unsigned Idx : seq<unsigned>(0, Sz)) {\n     if (Order[Idx] != Sz)\n@@ -7999,7 +8007,7 @@ BoUpSLP::collectUserStores(const BoUpSLP::TreeEntry *TE) const {\n       if (StoresVec.size() > Lane)\n         continue;\n       if (!StoresVec.empty()) {\n-        std::optional<int> Diff = getPointersDiff(\n+        std::optional<int64_t> Diff = getPointersDiff(\n             SI->getValueOperand()->getType(), SI->getPointerOperand(),\n             SI->getValueOperand()->getType(),\n             StoresVec.front()->getPointerOperand(), *DL, *SE,\n@@ -8027,14 +8035,14 @@ bool BoUpSLP::canFormVector(ArrayRef<StoreInst *> StoresVec,\n \n   // To avoid calling getPointersDiff() while sorting we create a vector of\n   // pairs {store, offset from first} and sort this instead.\n-  SmallVector<std::pair<int, unsigned>> StoreOffsetVec;\n+  SmallVector<std::pair<int64_t, unsigned>> StoreOffsetVec;\n   StoreInst *S0 = StoresVec[0];\n   StoreOffsetVec.emplace_back(0, 0);\n   Type *S0Ty = S0->getValueOperand()->getType();\n   Value *S0Ptr = S0->getPointerOperand();\n   for (unsigned Idx : seq<unsigned>(1, StoresVec.size())) {\n     StoreInst *SI = StoresVec[Idx];\n-    std::optional<int> Diff =\n+    std::optional<int64_t> Diff =\n         getPointersDiff(S0Ty, S0Ptr, SI->getValueOperand()->getType(),\n                         SI->getPointerOperand(), *DL, *SE,\n                         /*StrictCheck=*/true);\n@@ -8046,7 +8054,7 @@ bool BoUpSLP::canFormVector(ArrayRef<StoreInst *> StoresVec,\n     return false;\n   sort(StoreOffsetVec, llvm::less_first());\n   unsigned Idx = 0;\n-  int PrevDist = 0;\n+  int64_t PrevDist = 0;\n   for (const auto &P : StoreOffsetVec) {\n     if (Idx > 0 && P.first != PrevDist + 1)\n       return false;\n@@ -8130,15 +8138,15 @@ void BoUpSLP::buildTree(ArrayRef<Value *> Roots) {\n static void gatherPossiblyVectorizableLoads(\n     const BoUpSLP &R, ArrayRef<Value *> VL, const DataLayout &DL,\n     ScalarEvolution &SE, const TargetTransformInfo &TTI,\n-    SmallVectorImpl<SmallVector<std::pair<LoadInst *, int>>> &GatheredLoads,\n+    SmallVectorImpl<SmallVector<std::pair<LoadInst *, int64_t>>> &GatheredLoads,\n     bool AddNew = true) {\n   if (VL.empty())\n     return;\n   Type *ScalarTy = getValueType(VL.front());\n   if (!isValidElementType(ScalarTy))\n     return;\n-  SmallVector<SmallVector<std::pair<LoadInst *, int>>> ClusteredLoads;\n-  SmallVector<DenseMap<int, LoadInst *>> ClusteredDistToLoad;\n+  SmallVector<SmallVector<std::pair<LoadInst *, int64_t>>> ClusteredLoads;\n+  SmallVector<DenseMap<int64_t, LoadInst *>> ClusteredDistToLoad;\n   for (Value *V : VL) {\n     auto *LI = dyn_cast<LoadInst>(V);\n     if (!LI)\n@@ -8154,7 +8162,7 @@ static void gatherPossiblyVectorizableLoads(\n                                      RecursionMaxDepth) &&\n              \"Expected loads with the same type, same parent and same \"\n              \"underlying pointer.\");\n-      std::optional<int> Dist = getPointersDiff(\n+      std::optional<int64_t> Dist = getPointersDiff(\n           LI->getType(), LI->getPointerOperand(), Data.front().first->getType(),\n           Data.front().first->getPointerOperand(), DL, SE,\n           /*StrictCheck=*/true);\n@@ -8176,11 +8184,11 @@ static void gatherPossiblyVectorizableLoads(\n     }\n   }\n   auto FindMatchingLoads =\n-      [&](ArrayRef<std::pair<LoadInst *, int>> Loads,\n-          SmallVectorImpl<SmallVector<std::pair<LoadInst *, int>>>\n+      [&](ArrayRef<std::pair<LoadInst *, int64_t>> Loads,\n+          SmallVectorImpl<SmallVector<std::pair<LoadInst *, int64_t>>>\n               &GatheredLoads,\n           SetVector<unsigned> &ToAdd, SetVector<unsigned> &Repeated,\n-          int &Offset, unsigned &Start) {\n+          int64_t &Offset, unsigned &Start) {\n         if (Loads.empty())\n           return GatheredLoads.end();\n         LoadInst *LI = Loads.front().first;\n@@ -8191,16 +8199,16 @@ static void gatherPossiblyVectorizableLoads(\n           if (LI->getParent() != Data.front().first->getParent() ||\n               LI->getType() != Data.front().first->getType())\n             continue;\n-          std::optional<int> Dist =\n+          std::optional<int64_t> Dist =\n               getPointersDiff(LI->getType(), LI->getPointerOperand(),\n                               Data.front().first->getType(),\n                               Data.front().first->getPointerOperand(), DL, SE,\n                               /*StrictCheck=*/true);\n           if (!Dist)\n             continue;\n-          SmallSet<int, 4> DataDists;\n+          SmallSet<int64_t, 4> DataDists;\n           SmallPtrSet<LoadInst *, 4> DataLoads;\n-          for (std::pair<LoadInst *, int> P : Data) {\n+          for (std::pair<LoadInst *, int64_t> P : Data) {\n             DataDists.insert(P.second);\n             DataLoads.insert(P.first);\n           }\n@@ -8231,10 +8239,10 @@ static void gatherPossiblyVectorizableLoads(\n         ToAdd.clear();\n         return GatheredLoads.end();\n       };\n-  for (ArrayRef<std::pair<LoadInst *, int>> Data : ClusteredLoads) {\n+  for (ArrayRef<std::pair<LoadInst *, int64_t>> Data : ClusteredLoads) {\n     unsigned Start = 0;\n     SetVector<unsigned> ToAdd, LocalToAdd, Repeated;\n-    int Offset = 0;\n+    int64_t Offset = 0;\n     auto *It = FindMatchingLoads(Data, GatheredLoads, LocalToAdd, Repeated,\n                                  Offset, Start);\n     while (It != GatheredLoads.end()) {\n@@ -8249,7 +8257,7 @@ static void gatherPossiblyVectorizableLoads(\n           return !ToAdd.contains(Idx) && !Repeated.contains(Idx);\n         })) {\n       auto AddNewLoads =\n-          [&](SmallVectorImpl<std::pair<LoadInst *, int>> &Loads) {\n+          [&](SmallVectorImpl<std::pair<LoadInst *, int64_t>> &Loads) {\n             for (unsigned Idx : seq<unsigned>(Data.size())) {\n               if (ToAdd.contains(Idx) || Repeated.contains(Idx))\n                 continue;\n@@ -8259,7 +8267,7 @@ static void gatherPossiblyVectorizableLoads(\n       if (!AddNew) {\n         LoadInst *LI = Data.front().first;\n         It = find_if(\n-            GatheredLoads, [&](ArrayRef<std::pair<LoadInst *, int>> PD) {\n+            GatheredLoads, [&](ArrayRef<std::pair<LoadInst *, int64_t>> PD) {\n               return PD.front().first->getParent() == LI->getParent() &&\n                      PD.front().first->getType() == LI->getType();\n             });\n@@ -8267,7 +8275,7 @@ static void gatherPossiblyVectorizableLoads(\n           AddNewLoads(*It);\n           It = std::find_if(\n               std::next(It), GatheredLoads.end(),\n-              [&](ArrayRef<std::pair<LoadInst *, int>> PD) {\n+              [&](ArrayRef<std::pair<LoadInst *, int64_t>> PD) {\n                 return PD.front().first->getParent() == LI->getParent() &&\n                        PD.front().first->getType() == LI->getType();\n               });\n@@ -8280,9 +8288,10 @@ static void gatherPossiblyVectorizableLoads(\n }\n \n void BoUpSLP::tryToVectorizeGatheredLoads(\n-    const SmallMapVector<std::tuple<BasicBlock *, Value *, Type *>,\n-                         SmallVector<SmallVector<std::pair<LoadInst *, int>>>,\n-                         8> &GatheredLoads) {\n+    const SmallMapVector<\n+        std::tuple<BasicBlock *, Value *, Type *>,\n+        SmallVector<SmallVector<std::pair<LoadInst *, int64_t>>>, 8>\n+        &GatheredLoads) {\n   GatheredLoadsEntriesFirst = VectorizableTree.size();\n \n   SmallVector<SmallPtrSet<const Value *, 4>> LoadSetsToVectorize(\n@@ -8291,8 +8300,8 @@ void BoUpSLP::tryToVectorizeGatheredLoads(\n     Set.insert_range(VectorizableTree[Idx]->Scalars);\n \n   // Sort loads by distance.\n-  auto LoadSorter = [](const std::pair<LoadInst *, int> &L1,\n-                       const std::pair<LoadInst *, int> &L2) {\n+  auto LoadSorter = [](const std::pair<LoadInst *, int64_t> &L1,\n+                       const std::pair<LoadInst *, int64_t> &L2) {\n     return L1.second > L2.second;\n   };\n \n@@ -8454,28 +8463,30 @@ void BoUpSLP::tryToVectorizeGatheredLoads(\n   };\n   auto ProcessGatheredLoads =\n       [&, &TTI = *TTI](\n-          ArrayRef<SmallVector<std::pair<LoadInst *, int>>> GatheredLoads,\n+          ArrayRef<SmallVector<std::pair<LoadInst *, int64_t>>> GatheredLoads,\n           bool Final = false) {\n         SmallVector<LoadInst *> NonVectorized;\n-        for (ArrayRef<std::pair<LoadInst *, int>> LoadsDists : GatheredLoads) {\n+        for (ArrayRef<std::pair<LoadInst *, int64_t>> LoadsDists :\n+             GatheredLoads) {\n           if (LoadsDists.size() <= 1) {\n             NonVectorized.push_back(LoadsDists.back().first);\n             continue;\n           }\n-          SmallVector<std::pair<LoadInst *, int>> LocalLoadsDists(LoadsDists);\n+          SmallVector<std::pair<LoadInst *, int64_t>> LocalLoadsDists(\n+              LoadsDists);\n           SmallVector<LoadInst *> OriginalLoads(make_first_range(LoadsDists));\n           stable_sort(LocalLoadsDists, LoadSorter);\n           SmallVector<LoadInst *> Loads;\n           unsigned MaxConsecutiveDistance = 0;\n           unsigned CurrentConsecutiveDist = 1;\n-          int LastDist = LocalLoadsDists.front().second;\n+          int64_t LastDist = LocalLoadsDists.front().second;\n           bool AllowMaskedGather = IsMaskedGatherSupported(OriginalLoads);\n-          for (const std::pair<LoadInst *, int> &L : LocalLoadsDists) {\n+          for (const std::pair<LoadInst *, int64_t> &L : LocalLoadsDists) {\n             if (isVectorized(L.first))\n               continue;\n             assert(LastDist >= L.second &&\n                    \"Expected first distance always not less than second\");\n-            if (static_cast<unsigned>(LastDist - L.second) ==\n+            if (static_cast<uint64_t>(LastDist - L.second) ==\n                 CurrentConsecutiveDist) {\n               ++CurrentConsecutiveDist;\n               MaxConsecutiveDistance =\n@@ -8698,12 +8709,12 @@ void BoUpSLP::tryToVectorizeGatheredLoads(\n     if (!Ref.empty() && !NonVectorized.empty() &&\n         std::accumulate(\n             Ref.begin(), Ref.end(), 0u,\n-            [](unsigned S,\n-               ArrayRef<std::pair<LoadInst *, int>> LoadsDists) -> unsigned {\n-              return S + LoadsDists.size();\n-            }) != NonVectorized.size() &&\n+            [](unsigned S, ArrayRef<std::pair<LoadInst *, int64_t>> LoadsDists)\n+                -> unsigned { return S + LoadsDists.size(); }) !=\n+            NonVectorized.size() &&\n         IsMaskedGatherSupported(NonVectorized)) {\n-      SmallVector<SmallVector<std::pair<LoadInst *, int>>> FinalGatheredLoads;\n+      SmallVector<SmallVector<std::pair<LoadInst *, int64_t>>>\n+          FinalGatheredLoads;\n       for (LoadInst *LI : NonVectorized) {\n         // Reinsert non-vectorized loads to other list of loads with the same\n         // base pointers.\n@@ -9299,10 +9310,10 @@ BoUpSLP::TreeEntry::EntryState BoUpSLP::getScalarsVectorizationState(\n         Ptr0 = PointerOps[CurrentOrder.front()];\n         PtrN = PointerOps[CurrentOrder.back()];\n       }\n-      std::optional<int> Dist =\n+      std::optional<int64_t> Dist =\n           getPointersDiff(ScalarTy, Ptr0, ScalarTy, PtrN, *DL, *SE);\n       // Check that the sorted pointer operands are consecutive.\n-      if (static_cast<unsigned>(*Dist) == VL.size() - 1)\n+      if (static_cast<uint64_t>(*Dist) == VL.size() - 1)\n         return TreeEntry::Vectorize;\n     }\n \n@@ -10751,7 +10762,7 @@ unsigned BoUpSLP::canMapToVector(Type *T) const {\n \n   if (!isValidElementType(EltTy))\n     return 0;\n-  uint64_t VTSize = DL->getTypeStoreSizeInBits(getWidenedType(EltTy, N));\n+  size_t VTSize = DL->getTypeStoreSizeInBits(getWidenedType(EltTy, N));\n   if (VTSize < MinVecRegSize || VTSize > MaxVecRegSize ||\n       VTSize != DL->getTypeStoreSizeInBits(T))\n     return 0;\n@@ -11950,7 +11961,7 @@ void BoUpSLP::transformNodes() {\n   // A list of loads to be gathered during the vectorization process. We can\n   // try to vectorize them at the end, if profitable.\n   SmallMapVector<std::tuple<BasicBlock *, Value *, Type *>,\n-                 SmallVector<SmallVector<std::pair<LoadInst *, int>>>, 8>\n+                 SmallVector<SmallVector<std::pair<LoadInst *, int64_t>>>, 8>\n       GatheredLoads;\n \n   for (std::unique_ptr<TreeEntry> &TE : VectorizableTree) {\n@@ -18270,12 +18281,13 @@ Value *BoUpSLP::vectorizeTree(TreeEntry *E) {\n         Value *Ptr0 = cast<LoadInst>(E->Scalars.front())->getPointerOperand();\n         Value *PtrN = cast<LoadInst>(E->Scalars.back())->getPointerOperand();\n         PO = IsReverseOrder ? PtrN : Ptr0;\n-        std::optional<int> Diff = getPointersDiff(\n+        std::optional<int64_t> Diff = getPointersDiff(\n             VL0->getType(), Ptr0, VL0->getType(), PtrN, *DL, *SE);\n         Type *StrideTy = DL->getIndexType(PO->getType());\n         Value *StrideVal;\n         if (Diff) {\n-          int Stride = *Diff / (static_cast<int>(E->Scalars.size()) - 1);\n+          int64_t Stride =\n+              *Diff / (static_cast<int64_t>(E->Scalars.size()) - 1);\n           StrideVal =\n               ConstantInt::get(StrideTy, (IsReverseOrder ? -1 : 1) * Stride *\n                                              DL->getTypeAllocSize(ScalarTy));\n@@ -21133,18 +21145,18 @@ public:\n   /// \\p PtrDist.\n   /// Does nothing if there is already a store with that \\p PtrDist.\n   /// \\returns The previously associated Instruction index, or std::nullopt\n-  std::optional<unsigned> insertOrLookup(unsigned InstrIdx, int PtrDist) {\n+  std::optional<unsigned> insertOrLookup(unsigned InstrIdx, int64_t PtrDist) {\n     auto [It, Inserted] = Instrs.emplace(PtrDist, InstrIdx);\n-    return Inserted ? std::nullopt : std::optional<unsigned>(It->second);\n+    return Inserted ? std::nullopt : std::make_optional(It->second);\n   }\n \n-  using DistToInstMap = std::map<int, unsigned>;\n+  using DistToInstMap = std::map<int64_t, unsigned>;\n   const DistToInstMap &getStores() const { return Instrs; }\n \n   /// If \\p SI is related to this group of stores, return the distance of its\n   /// pointer operand to the one the group's BaseInstr.\n-  std::optional<int> getPointerDiff(StoreInst &SI, const DataLayout &DL,\n-                                    ScalarEvolution &SE) const {\n+  std::optional<int64_t> getPointerDiff(StoreInst &SI, const DataLayout &DL,\n+                                        ScalarEvolution &SE) const {\n     StoreInst &BaseStore = *AllStores[BaseInstrIdx];\n     return getPointersDiff(\n         BaseStore.getValueOperand()->getType(), BaseStore.getPointerOperand(),\n@@ -21155,7 +21167,7 @@ public:\n   /// Recompute the pointer distances to be based on \\p NewBaseInstIdx.\n   /// Stores whose index is less than \\p MinSafeIdx will be dropped.\n   void rebase(unsigned MinSafeIdx, unsigned NewBaseInstIdx,\n-              int DistFromCurBase) {\n+              int64_t DistFromCurBase) {\n     DistToInstMap PrevSet = std::move(Instrs);\n     reset(NewBaseInstIdx);\n \n@@ -21171,7 +21183,7 @@ public:\n   /// Remove all stores that have been vectorized from this group.\n   void clearVectorizedStores(const BoUpSLP::ValueSet &VectorizedStores) {\n     DistToInstMap::reverse_iterator LastVectorizedStore = find_if(\n-        reverse(Instrs), [&](const std::pair<int, unsigned> &DistAndIdx) {\n+        reverse(Instrs), [&](const std::pair<int64_t, unsigned> &DistAndIdx) {\n           return VectorizedStores.contains(AllStores[DistAndIdx.second]);\n         });\n \n@@ -21204,7 +21216,7 @@ bool SLPVectorizerPass::vectorizeStores(\n   bool Changed = false;\n \n   auto TryToVectorize = [&](const RelatedStoreInsts::DistToInstMap &StoreSeq) {\n-    int PrevDist = -1;\n+    int64_t PrevDist = -1;\n     BoUpSLP::ValueList Operands;\n     // Collect the chain into a list.\n     for (auto [Idx, Data] : enumerate(StoreSeq)) {\n@@ -21505,7 +21517,7 @@ bool SLPVectorizerPass::vectorizeStores(\n   // dependencies and no need to waste compile time to try to vectorize them.\n   // - Try to vectorize the sequence {1, {1, 0}, {3, 2}}.\n   auto FillStoresSet = [&](unsigned Idx, StoreInst *SI) {\n-    std::optional<int> PtrDist;\n+    std::optional<int64_t> PtrDist;\n     auto *RelatedStores = find_if(\n         SortedStores, [&PtrDist, SI, this](const RelatedStoreInsts &StoreSeq) {\n           PtrDist = StoreSeq.getPointerDiff(*SI, *DL, *SE);\n",
  "tests": [
    {
      "file": "llvm/test/Transforms/SLPVectorizer/X86/long-pointer-distance.ll",
      "commands": [
        "opt -S --passes=slp-vectorizer -mtriple=x86_64-grtev4-linux-gnu < %s -mattr=+avx"
      ],
      "tests": [
        {
          "test_name": "test",
          "test_body": "define void @test(ptr %this) {\nentry:\n  store i64 1, ptr %this, align 8\n  %b = getelementptr i8, ptr %this, i64 8\n  store i64 2, ptr %b, align 8\n  %c = getelementptr i8, ptr %this, i64 4294967312\n  store i64 3, ptr %c, align 8\n  %d = getelementptr i8, ptr %this, i64 4294967320\n  store i64 4, ptr %d, align 8\n  ret void\n}\n"
        }
      ]
    }
  ],
  "issue": {
    "title": "[SLPV] Pointer offsets truncated to 32-bits, resulting in wrong code",
    "body": "Consider:\n\n```\n$ cat t.ll\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-grtev4-linux-gnu\"\n\ndefine void @_ZN12_GLOBAL__N_111LargeObjectILm4294967296EEC2Em(ptr %this) #0 {\nentry:\n  store i64 1, ptr %this, align 8\n  %b = getelementptr i8, ptr %this, i64 8\n  store i64 2, ptr %b, align 8\n  %c = getelementptr i8, ptr %this, i64 u0x100000010\n  store i64 3, ptr %c, align 8\n  %d = getelementptr i8, ptr %this, i64 u0x100000018\n  store i64 4, ptr %d, align 8\n  ret void\n}\n\nattributes #0 = { \"target-features\"=\"+aes,+avx,+cmov,+crc32,+cx16,+cx8,+fxsr,+mmx,+pclmul,+popcnt,+prfchw,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave\" }\n\n$ opt -passes=slp-vectorizer t.ll  -S -o -\n; ModuleID = 't.ll'\nsource_filename = \"t.ll\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-grtev4-linux-gnu\"\n\ndefine void @_ZN12_GLOBAL__N_111LargeObjectILm4294967296EEC2Em(ptr %this) #0 {\nentry:\n  store <4 x i64> <i64 1, i64 2, i64 3, i64 4>, ptr %this, align 8\n  ret void\n}\n\nattributes #0 = { \"target-features\"=\"+aes,+avx,+cmov,+crc32,+cx16,+cx8,+fxsr,+mmx,+pclmul,+popcnt,+prfchw,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave\" }\n```\n\nClearly, somewhere inside SLPV, the GEP offsets here are truncated from 64-bits to 32. To quickly check SLPV with static analysis, I did this:\n\n```\n$ ninja opt # build everything once\n$ touch ../llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp\n$ CCC_OVERRIDE_OPTIONS='+-Wshorten-64-to-32' ninja opt | tee slpv-truncations.txt\n...\n```\n\nUnfortunately, there are 733 hits:\n```\n$ grep warning: slpv-truncations.txt  | wc -l\n     733\n```\n\nThis is not a recent regression, it goes back to LLVM 14 and earlier: https://godbolt.org/z/9baE551T7 LLVM 14 is just as far back as the IR is parseable out of the box.\n\nI think the next move here would be to do a global search & replace of getZExtValue with some variant that asserts when the result is wider than 32-bits, or to focus only on the warnings that include getZExtValue in the warning text snippet.",
    "author": "rnk",
    "labels": [
      "miscompilation",
      "llvm:SLPVectorizer",
      "llvm:analysis"
    ],
    "comments": [
      {
        "author": "alexey-bataev",
        "body": "LoopAccessAnalysis.cpp, getPointersDiff() returns distance -16 for pointers %b and %d, there is an implicit cast from int64_t to int, which looses the info\nhttps://github.com/llvm/llvm-project/blob/9eeae5a5de41b22d31d9037be2acc45dd7af4192/llvm/lib/Analysis/LoopAccessAnalysis.cpp#L1598\nhttps://github.com/llvm/llvm-project/blob/9eeae5a5de41b22d31d9037be2acc45dd7af4192/llvm/lib/Analysis/LoopAccessAnalysis.cpp#L1589\n"
      }
    ]
  },
  "verified": true,
  "properties": {
    "is_single_file_fix": false,
    "is_single_func_fix": false
  },
  "bisect": "Cannot find a good commit"
}